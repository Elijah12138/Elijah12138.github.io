[{"id":"2748178e20072a32602a87f0ad5f4d49","title":"YOLOV5-ONNX-Flask","content":"YOLOV5-ONNX-Flask项目部署\n\n\n\n\n\n\n\n\nPytorch导出ONNX模型可以参考：\nhttps://elijah12138.github.io/post/YOLOV5-Pytorch\n1. 下载项目到本地\n\n\n\n\n\n\n\n\ngithub地址:\n2. templates文件夹保存网页信息index.html\n&lt;html>\n  &lt;head>\n    &lt;title>YOLOV5-ONNX-Flask项目部署&lt;/title>\n  &lt;/head>\n  &lt;body>\n    &lt;h1>YOLOV5-ONNX-Flask项目部署&lt;/h1>\n    &lt;img src=\"&#123;&#123; url_for('video_feed') &#125;&#125;\" width=\"960px\" height=\"720px\"/>\n  &lt;/body>\n&lt;/html>\n\n3. class.names文件夹下保存待检测的类别person\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\nfire hydrant\nstop sign\nparking meter\nbench\nbird\ncat\ndog\nhorse\nsheep\ncow\nelephant\nbear\nzebra\ngiraffe\nbackpack\numbrella\nhandbag\ntie\nsuitcase\nfrisbee\nskis\nsnowboard\nsports ball\nkite\nbaseball bat\nbaseball glove\nskateboard\nsurfboard\ntennis racket\nbottle\nwine glass\ncup\nfork\nknife\nspoon\nbowl\nbanana\napple\nsandwich\norange\nbroccoli\ncarrot\nhot dog\npizza\ndonut\ncake\nchair\ncouch\npotted plant\nbed\ndining table\ntoilet\ntv\nlaptop\nmouse\nremote\nkeyboard\ncell phone\nmicrowave\noven\ntoaster\nsink\nrefrigerator\nbook\nclock\nvase\nscissors\nteddy bear\nhair drier\ntoothbrush\n\n4. v5_dnn.py 保存调用ONNX模型进行检测的方法import cv2\nimport numpy as np\n\nclass yolov5():\n    def __init__(self, modelpath, confThreshold=0.5, nmsThreshold=0.5, objThreshold=0.5):\n        with open('class.names', 'rt') as f:\n            self.classes = f.read().rstrip('\\n').split('\\n')\n        self.num_classes = len(self.classes)\n        if modelpath.endswith('6.onnx'):\n            self.inpHeight, self.inpWidth = 1280, 1280\n            anchors = [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542],\n                       [436, 615, 739, 380, 925, 792]]\n            self.stride = np.array([8., 16., 32., 64.])\n        else:\n            self.inpHeight, self.inpWidth = 640, 640\n            anchors = [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]\n            self.stride = np.array([8., 16., 32.])\n        self.nl = len(anchors)\n        self.na = len(anchors[0]) // 2\n        self.grid = [np.zeros(1)] * self.nl\n        self.anchor_grid = np.asarray(anchors, dtype=np.float32).reshape(self.nl, -1, 2)\n        self.net = cv2.dnn.readNet(modelpath)\n        self.confThreshold = confThreshold\n        self.nmsThreshold = nmsThreshold\n        self.objThreshold = objThreshold\n        self._inputNames = ''\n\n\n    def resize_image(self, srcimg, keep_ratio=True, dynamic=False):\n        top, left, newh, neww = 0, 0, self.inpWidth, self.inpHeight\n        if keep_ratio and srcimg.shape[0] != srcimg.shape[1]:\n            hw_scale = srcimg.shape[0] / srcimg.shape[1]\n            if hw_scale > 1:\n                newh, neww = self.inpHeight, int(self.inpWidth / hw_scale)\n                img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA)\n                if not dynamic:\n                    left = int((self.inpWidth - neww) * 0.5)\n                    img = cv2.copyMakeBorder(img, 0, 0, left, self.inpWidth - neww - left, cv2.BORDER_CONSTANT,\n                                             value=(114, 114, 114))  # add border\n            else:\n                newh, neww = int(self.inpHeight * hw_scale), self.inpWidth\n                img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA)\n                if not dynamic:\n                    top = int((self.inpHeight - newh) * 0.5)\n                    img = cv2.copyMakeBorder(img, top, self.inpHeight - newh - top, 0, 0, cv2.BORDER_CONSTANT,\n                                             value=(114, 114, 114))\n        else:\n            img = cv2.resize(srcimg, (self.inpWidth, self.inpHeight), interpolation=cv2.INTER_AREA)\n        return img, newh, neww, top, left\n\n    def _make_grid(self, nx=20, ny=20):\n        xv, yv = np.meshgrid(np.arange(ny), np.arange(nx))\n        return np.stack((xv, yv), 2).reshape((-1, 2)).astype(np.float32)\n\n    def preprocess(self, img):\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32) / 255.0\n        return img\n\n    def postprocess(self, frame, outs, padsize=None):\n        frameHeight = frame.shape[0]\n        frameWidth = frame.shape[1]\n        newh, neww, padh, padw = padsize\n        ratioh, ratiow = frameHeight / newh, frameWidth / neww\n        # Scan through all the bounding boxes output from the network and keep only the\n        # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n\n        confidences = []\n        boxes = []\n        classIds = []\n        for detection in outs:\n            if detection[4] > self.objThreshold:\n                scores = detection[5:]\n                classId = np.argmax(scores)\n                confidence = scores[classId] * detection[4]\n                if confidence > self.confThreshold:\n                    center_x = int((detection[0] - padw) * ratiow)\n                    center_y = int((detection[1] - padh) * ratioh)\n                    width = int(detection[2] * ratiow)\n                    height = int(detection[3] * ratioh)\n                    left = int(center_x - width * 0.5)\n                    top = int(center_y - height * 0.5)\n\n                    confidences.append(float(confidence))\n                    boxes.append([left, top, width, height])\n                    classIds.append(classId)\n        # Perform non maximum suppression to eliminate redundant overlapping boxes with\n        # lower confidences.\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confThreshold, self.nmsThreshold)\n        \n        for i in indices:\n            box = boxes[i]\n            left = box[0]\n            top = box[1]\n            width = box[2]\n            height = box[3]\n            frame = self.drawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height)\n        return frame\n\n    def drawPred(self, frame, classId, conf, left, top, right, bottom):\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), thickness=4)\n\n        label = '%.2f' % conf\n        label = '%s:%s' % (self.classes[classId], label)\n\n        # Display the label at the top of the bounding box\n        labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        top = max(top, labelSize[1])\n        # cv.rectangle(frame, (left, top - round(1.5 * labelSize[1])), (left + round(1.5 * labelSize[0]), top + baseLine), (255,255,255), cv.FILLED)\n        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), thickness=2)\n\n        return frame\n\n    def detect(self, srcimg):\n        img, newh, neww, padh, padw = self.resize_image(srcimg)\n        blob = cv2.dnn.blobFromImage(img, scalefactor=1 / 255.0, swapRB=True)\n        # blob = cv2.dnn.blobFromImage(self.preprocess(img))\n        # Sets the input to the network\n        self.net.setInput(blob, self._inputNames)\n\n        # Runs the forward pass to get output of the output layers\n        outs = self.net.forward(self.net.getUnconnectedOutLayersNames())[0].squeeze(axis=0)\n\n        # inference output\n        row_ind = 0\n        for i in range(self.nl):\n            h, w = int(self.inpHeight / self.stride[i]), int(self.inpWidth / self.stride[i])\n            length = int(self.na * h * w)\n            if self.grid[i].shape[2:4] != (h, w):\n                self.grid[i] = self._make_grid(w, h)\n\n            outs[row_ind:row_ind + length, 0:2] = (outs[row_ind:row_ind + length, 0:2] * 2. - 0.5 + np.tile(\n                self.grid[i], (self.na, 1))) * int(self.stride[i])\n            outs[row_ind:row_ind + length, 2:4] = (outs[row_ind:row_ind + length, 2:4] * 2) ** 2 * np.repeat(\n                self.anchor_grid[i], h * w, axis=0)\n            row_ind += length\n\n        srcimg = self.postprocess(srcimg, outs, padsize=(newh, neww, padh, padw))\n        return srcimg\n\nif __name__ == \"__main__\":\n    modelpath = 'yolov5n.onnx'\n    confThreshold = 0.3\n    nmsThreshold = 0.5\n    objThreshold = 0.3\n\n    yolonet = yolov5(modelpath, confThreshold=confThreshold, nmsThreshold=nmsThreshold,\n                     objThreshold=objThreshold)\n\n    winName = 'Deep learning object detection in OpenCV'\n    cv2.namedWindow(winName, 0)\n    cap = cv2.VideoCapture(0)\n    cap.set(3, 960)  # set video width\n    cap.set(4, 780)  # set video height\n    while True:\n        hasMoreFrame, frame = cap.read()\n        if hasMoreFrame == True:\n            srcimg = yolonet.detect(frame)\n            cv2.imshow(winName, srcimg)\n            k = cv2.waitKey(20)\n            # q键退出\n            if (k &amp; 0xff == ord('q')):\n                break\n    cap.release()\n    cv2.destroyAllWindows()\n\n5. app.py保存程序启动信息from operator import ne\nfrom flask import Flask, render_template, Response\nfrom v5_dnn import *\nimport time\nfrom cv2 import getTickCount, getTickFrequency\n\nclass VideoCamera(object):\n    def __init__(self):\n        self.video = cv2.VideoCapture(0)\n\n    def __del__(self):\n        self.video.release()\n    def get_frame(self):\n        success, image = self.video.read()\n        return image\n\napp = Flask(__name__)\n\n@app.route('/')  # 主页\ndef index():\n    return render_template('index.html')\n\n\ndef v5_dnn(camera):\n    modelpath = 'yolov5n.onnx'\n    confThreshold = 0.3\n    nmsThreshold = 0.5\n    objThreshold = 0.3\n    yolonet = yolov5(modelpath, confThreshold=confThreshold, nmsThreshold=nmsThreshold,\n                     objThreshold=objThreshold)\n    while True:\n        start = time.clock()\n        frame = camera.get_frame()\n        yolonet.detect(frame)\n        end = time.clock()\n        fps = 1. / (end - start)\n        cv2.putText(frame, \"fps= %.2f\" % (fps), (0, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n        ret, jpeg = cv2.imencode('.jpg', frame)\n        frame = jpeg.tobytes()\n        yield (b'--frame\\r\\n'\n               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')\n\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(v5_dnn(VideoCamera()),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', debug=True, port=5000)\n\n","slug":"YOLOV5-ONNX-Flask","date":"2022-07-05T12:46:54.000Z","categories_index":"目标检测,机器学习,深度学习,yolo","tags_index":"机器学习,深度学习,目标检测,yolo,ONNX,Flask","author_index":"Elijah"},{"id":"3e82326fbfb3cfc01a29581e5034be52","title":"YOLOV5-Pytorch","content":"YOLOV5-Pytorch1. 下载官方代码并进行测试1.1 克隆github项目到本地# 已更新到v6.1版本\ngit clone https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov5.git\n\n1.2 安装所需库pip install -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple -r requirements.txt\n\n\n\n\n\n\n\n\n\n\n测试环境: \npython: 3.7.2\ntorch: 1.8.1+cu101\ntorchvision: 0.9.1+cu101\ntensorboard: 2.9.1\n1.3 下载预训练权重文件\n\n\n\n\n\n\n\n\n创建一个文件夹weights，用于保存下载的权重文件：\n\n\n\n\n\n\n\n\n\n官网下载地址：\nhttps://github.com/ultralytics/yolov5/releases\n本地如果没有下载权重文件，在运行detect.py时程序也会自动下载权重文件。\n\n1.4 测试检测图片进入yolov5文件夹下，在cmd下运行命令：\n# 以yolov5s.pt为例\npython detect.py --weights weights&#x2F;yolov5s.pt --img 640 --conf 0.25 --source bus.jpg --data data&#x2F;coco.yaml\n\n\n1.5 测试检测视频python detect.py --source data&#x2F;test_video.mp4 --weights weights&#x2F;yolov5n.pt  --data data&#x2F;coco.yaml\n\n\n\n2. 制作自己的数据集2.1 重命名图片from hashlib import new\nimport os\n\ndef rename_files(path):\n    filelist = os.listdir(path)\n    filelist.sort()\n    total_num = len(filelist)\n    print(\"total number: &#123;&#125;\".format(total_num))\n    i = 1\n    for item in filelist:\n        if item.endswith('.jpg'):\n            original_name = os.path.join(path, item)\n            try:\n                new_name = os.path.join(path, str(i).zfill(4) + \".jpg\")\n                i = i + 1\n                os.rename(original_name, new_name)\n                print(\"&#123;&#125; rename ----> &#123;&#125;\".format(original_name, new_name))\n            except Exception as e:\n                print(e)\n                print(\"rename dir fail\")\n\npath = \"./data\"\n\nrename_files(path)\n\n\n2.2 缩小图片数据\n\n\n\n\n\n\n\n\n防止出现图片数据过大，显存溢出的现象。\nimport os\nfrom PIL import Image\n\ndef resize_images(dirPath, new_dirPath):\n    fileName_list = os.listdir(dirPath)\n    filePath_list = [os.path.join(dirPath, fileName) for fileName in fileName_list]\n    imagePath_list = [filePath for filePath in filePath_list if '.jpg' in filePath]\n    if not os.path.isdir(new_dirPath):\n        os.mkdir(new_dirPath)\n    for imagePath in imagePath_list:\n        image = Image.open(imagePath)\n        width, height = image.size\n        imageName = imagePath.split('\\\\')[-1]\n        save_path = os.path.join(new_dirPath, imageName)\n        if width >= 600 and height >= 600:\n            minification = min(width, height) // 300 #此变量表示缩小倍数\n            new_width = width // minification\n            new_height = height // minification\n            resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n            print('图片&#123;&#125;原来的宽&#123;&#125;, 高&#123;&#125;, -------->  图片缩小后宽&#123;&#125;, 高&#123;&#125;' .format(imageName, width, height, new_width, new_height))\n            resized_image.save(save_path)\n        else:\n            image.save(save_path)\n\nresize_images('data', 'processed_data')\n\n\n\n2.3 安装labelImg# 安装labelImg\npip install labelImg\n\n# 启动labelImg\nlabelImg\n\n2.4 标记数据\n点击Open Dir选择图片数据所在的文件夹，标记格式选择VOC格式；\n\n\n按一下w键，即可标记目标数据，选定区域后输入所属的标签；\n\n\n按住ctrl+s保存标记的信息；\n\n\n切换图片，标记下一张。\n\n\n\n3. 训练数据3.1 修改文件夹结构\n\n\n\n\n\n\n\n\n在yolov5文件夹下创建VOCdevkit &#x2F; VOC2007文件夹，VOC2007下面建立两个文件夹：Annotations和JPEGImages。\nJPEGImages放所有的训练和测试图片；Annotations放所有的xml标记文件\n3.2 预处理训练数据在yolov5文件夹里运行代码：\nimport xml.etree.ElementTree as ET\nimport pickle\nimport os\nfrom os import listdir, getcwd\nfrom os.path import join\nimport random\nfrom shutil import copyfile\n\n# 根据自己的数据标签修改\nclasses=[\"chicken_head\"]\n\n\ndef clear_hidden_files(path):\n    dir_list = os.listdir(path)\n    for i in dir_list:\n        abspath = os.path.join(os.path.abspath(path), i)\n        if os.path.isfile(abspath):\n            if i.startswith(\"._\"):\n                os.remove(abspath)\n        else:\n            clear_hidden_files(abspath)\n\ndef convert(size, box):\n    dw = 1./size[0]\n    dh = 1./size[1]\n    x = (box[0] + box[1])/2.0\n    y = (box[2] + box[3])/2.0\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    x = x*dw\n    w = w*dw\n    y = y*dh\n    h = h*dh\n    return (x,y,w,h)\n\ndef convert_annotation(image_id):\n    in_file = open('VOCdevkit/VOC2007/Annotations/%s.xml' %image_id)\n    out_file = open('VOCdevkit/VOC2007/YOLOLabels/%s.txt' %image_id, 'w')\n    tree=ET.parse(in_file)\n    root = tree.getroot()\n    size = root.find('size')\n    w = int(size.find('width').text)\n    h = int(size.find('height').text)\n\n    for obj in root.iter('object'):\n        difficult = obj.find('difficult').text\n        cls = obj.find('name').text\n        if cls not in classes or int(difficult) == 1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find('bndbox')\n        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n        bb = convert((w,h), b)\n        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n    in_file.close()\n    out_file.close()\n\nwd = os.getcwd()\nwd = os.getcwd()\ndata_base_dir = os.path.join(wd, \"VOCdevkit/\")\nif not os.path.isdir(data_base_dir):\n    os.mkdir(data_base_dir)\nwork_sapce_dir = os.path.join(data_base_dir, \"VOC2007/\")\nif not os.path.isdir(work_sapce_dir):\n    os.mkdir(work_sapce_dir)\nannotation_dir = os.path.join(work_sapce_dir, \"Annotations/\")\nif not os.path.isdir(annotation_dir):\n        os.mkdir(annotation_dir)\nclear_hidden_files(annotation_dir)\nimage_dir = os.path.join(work_sapce_dir, \"JPEGImages/\")\nif not os.path.isdir(image_dir):\n        os.mkdir(image_dir)\nclear_hidden_files(image_dir)\nyolo_labels_dir = os.path.join(work_sapce_dir, \"YOLOLabels/\")\nif not os.path.isdir(yolo_labels_dir):\n        os.mkdir(yolo_labels_dir)\nclear_hidden_files(yolo_labels_dir)\nyolov5_images_dir = os.path.join(data_base_dir, \"images/\")\nif not os.path.isdir(yolov5_images_dir):\n        os.mkdir(yolov5_images_dir)\nclear_hidden_files(yolov5_images_dir)\nyolov5_labels_dir = os.path.join(data_base_dir, \"labels/\")\nif not os.path.isdir(yolov5_labels_dir):\n        os.mkdir(yolov5_labels_dir)\nclear_hidden_files(yolov5_labels_dir)\nyolov5_images_train_dir = os.path.join(yolov5_images_dir, \"train/\")\nif not os.path.isdir(yolov5_images_train_dir):\n        os.mkdir(yolov5_images_train_dir)\nclear_hidden_files(yolov5_images_train_dir)\nyolov5_images_test_dir = os.path.join(yolov5_images_dir, \"val/\")\nif not os.path.isdir(yolov5_images_test_dir):\n        os.mkdir(yolov5_images_test_dir)\nclear_hidden_files(yolov5_images_test_dir)\nyolov5_labels_train_dir = os.path.join(yolov5_labels_dir, \"train/\")\nif not os.path.isdir(yolov5_labels_train_dir):\n        os.mkdir(yolov5_labels_train_dir)\nclear_hidden_files(yolov5_labels_train_dir)\nyolov5_labels_test_dir = os.path.join(yolov5_labels_dir, \"val/\")\nif not os.path.isdir(yolov5_labels_test_dir):\n        os.mkdir(yolov5_labels_test_dir)\nclear_hidden_files(yolov5_labels_test_dir)\n\ntrain_file = open(os.path.join(wd, \"yolov5_train.txt\"), 'w')\ntest_file = open(os.path.join(wd, \"yolov5_val.txt\"), 'w')\ntrain_file.close()\ntest_file.close()\ntrain_file = open(os.path.join(wd, \"yolov5_train.txt\"), 'a')\ntest_file = open(os.path.join(wd, \"yolov5_val.txt\"), 'a')\nlist_imgs = os.listdir(image_dir) # list image files\nprobo = random.randint(1, 100)\nprint(\"Probobility: %d\" % probo)\nfor i in range(0,len(list_imgs)):\n    path = os.path.join(image_dir,list_imgs[i])\n    if os.path.isfile(path):\n        image_path = image_dir + list_imgs[i]\n        voc_path = list_imgs[i]\n        (nameWithoutExtention, extention) = os.path.splitext(os.path.basename(image_path))\n        (voc_nameWithoutExtention, voc_extention) = os.path.splitext(os.path.basename(voc_path))\n        annotation_name = nameWithoutExtention + '.xml'\n        annotation_path = os.path.join(annotation_dir, annotation_name)\n        label_name = nameWithoutExtention + '.txt'\n        label_path = os.path.join(yolo_labels_dir, label_name)\n    probo = random.randint(1, 100)\n    print(\"Probobility: %d\" % probo)\n    if(probo &lt; 80): # train dataset\n        if os.path.exists(annotation_path):\n            train_file.write(image_path + '\\n')\n            convert_annotation(nameWithoutExtention) # convert label\n            copyfile(image_path, yolov5_images_train_dir + voc_path)\n            copyfile(label_path, yolov5_labels_train_dir + label_name)\n    else: # test dataset\n        if os.path.exists(annotation_path):\n            test_file.write(image_path + '\\n')\n            convert_annotation(nameWithoutExtention) # convert label\n            copyfile(image_path, yolov5_images_test_dir + voc_path)\n            copyfile(label_path, yolov5_labels_test_dir + label_name)\ntrain_file.close()\ntest_file.close()\n\n3.3 修改配置文件\n\n\n\n\n\n\n\n\n新建文件data/voc-chicken.yaml， 内容如下：\ntrain: ..&#x2F;yolov5&#x2F;VOCdevkit&#x2F;images&#x2F;train&#x2F;\nval: ..&#x2F;yolov5&#x2F;VOCdevkit&#x2F;images&#x2F;val&#x2F;\n\n# number of classes\nnc: 1\n\n# class names\nnames: [&#39;chicken_head&#39;]\n\n\n\n\n\n\n\n\n\n\n复制models&#x2F;yolov5s.yaml重命名为models&#x2F;yolov5s-chicken.yaml 然后修改配置参数：\n# Parameters\nnc: 1  # number of classes\n\n3.4 训练数据python train.py --data data&#x2F;voc-chicken.yaml --cfg models&#x2F;yolov5s-chicken.yaml --weights weights&#x2F;yolov5s.pt --epochs 500 --device 0 --batch-size 16\n\n\n\n4. 训练数据可视化以及性能统计# 安装Tensorboard\npip install tensorboard\n\n# 启动Tensorboard\ntensorboard --logdir&#x3D;.&#x2F;runs\n\n# 启动后，在浏览器输入http:&#x2F;&#x2F;localhost:6006&#x2F;即可\n\n\n\n5. 测试训练效果python detect.py --weights weights&#x2F;best.pt --img 640 --conf 0.25 --source pic1.png --data data&#x2F;voc-chicken.yaml\n\n\n# 测试模型效果\npython val.py --weights weights&#x2F;best.pt --img 640 --conf 0.25 --data data&#x2F;voc-chicken.yaml --batch-size 8\n\n\n6. 使用ONNX格式并进行测试\n\n\n\n\n\n\n\n\n查看ONNX模型的结构：\nhttps://netron.app/\n6.1 将pt模型转换为onnx模型python export.py --weights weights/best.pt --img 640 --batch 1 --opset 12\n\n6.2 通过OpenCV使用ONNX模型python detect.py --weights weights&#x2F;best.onnx --dnn --source pic1.png --img 640 --data data&#x2F;voc-chicken.yaml\n\n\n\n","slug":"YOLOV5-Pytorch","date":"2022-07-03T12:46:54.000Z","categories_index":"目标检测,机器学习,深度学习,yolo","tags_index":"机器学习,深度学习,目标检测,yolo","author_index":"Elijah"},{"id":"85cf65706b3185fdb70607bc9be65855","title":"Matplotlib常用方法","content":"Matplotlib常用方法1. Pyplot1.1 根据xy数据绘制默认图像import matplotlib.pyplot as plt\nimport numpy as np\n\ny = np.random.randint(10, 20, size=10)\n\nplt.plot(y)\nplt.show()\n\n1.2 更改绘图标记1.2.1 常用的绘图标记\n\n\n绘图标记\n\n\n\n\n\n颜色字符\nb\n蓝色\n\n\n\nm\n洋红色\n\n\n\ng\n绿色\n\n\n\ny\n黄色\n\n\n\nr\n红色\n\n\n\nk\n黑色\n\n\n\nw\n白色\n\n\n\nc\n青绿色\n\n\n\n#FFD700\n金色\n\n\n线型参数\n-\n实线\n\n\n\n–\n破折线\n\n\n\n-.\n点划线\n\n\n\n:\n虚线\n\n\n标记字符\n.\n点标记\n\n\n\n,\n像素标记\n\n\n\no\n实心圈标记\n\n\n\nv\n倒三角标记\n\n\n\n^\n上三角标记\n\n\n\n&gt;\n右三角标记\n\n\n\n&lt;\n左三角标记\n\n\n1.2.2 样例plt.plot(x, y, 'g:v')\nplt.show()\n\n# 只绘制点，不绘制线\nplt.plot(x, y, 'gv')\nplt.show()\n\n2. 轴标签和标题2.1 设置轴标题plt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20)\nplt.plot(x, y, 'g:v')\nplt.show()\n\n\n\n2.2 设置标题plt.title(\"title\", fontsize=30)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20)\nplt.plot(x, y, 'g:v')\nplt.show()\n\n\n\n3. 网格线plt.title(\"title\", fontsize=30)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20)\nplt.plot(x, y, 'g:v')\n# axis: 只设置x轴上的网格线\n# color: 设置网格线的颜色\n# linestyle: 设置网格线的样式\n# linewidth: 设置网格线的间隔\nplt.grid(axis='x', color = 'y', linestyle = '--', linewidth = 2)\nplt.show()\n\n\n\n4. 绘制多图x = np.arange(1, 11, 1)\ny_1 = np.random.randint(10, 20, size=10)\ny_2 = np.random.randint(10, 20, size=10)\ny_3 = np.random.randint(15, 25, size=10)\ny_4 = np.random.randint(15, 30, size=10)\n\nplt.subplot(2, 2, 1)\nplt.plot(x, y_1)\nplt.title(\"plot 1\")\n\nplt.subplot(2, 2, 2)\nplt.plot(x, y_2)\nplt.title(\"plot 2\")\n\nplt.subplot(2, 2, 3)\nplt.plot(x, y_3)\nplt.title(\"plot 3\")\n\nplt.subplot(2, 2, 4)\nplt.plot(x, y_4)\nplt.title(\"plot 4\")\n\nplt.suptitle(\"subplot demo\")\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\nplt.show()\n\n5. 显示中文# 支持中文\nplt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\nplt.title(\"图表\", fontsize=30)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20)\nplt.plot(x, y, 'g:v')\n# axis: 只设置x轴上的网格线\n# color: 设置网格线的颜色\n# linestyle: 设置网格线的样式\n# linewidth: 设置网格线的间隔\nplt.grid(axis='x', color = 'y', linestyle = '--', linewidth = 2)\nplt.show()\n\n6. 常用的图表6.1 散点图plt.title(\"散点图\", fontsize=30)\n# color: 设置点的颜色\n# alpha: 设置点的透明度\n# s: 设置点的大小\nplt.scatter(x, y_1, color='y')\nplt.scatter(x, y_2, color='r', alpha=0.5, s=500)\nplt.show()\n\n\n\n6.2. 柱形图plt.title(\"柱形图\", fontsize=30)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20)\n# 纵向的柱形图\nplt.bar(x, y, color=['y', 'r'], width=0.5)\n\n# 横向的柱形图\n# plt.barh(x, y, color=['y', 'r'], height=0.5)\nplt.show()\n\n\n\n6.3. 饼图plt.title(\"饼图\", fontsize=30)\ny = np.random.randint(10, 20, size=5)\nplt.pie(y,\n    labels=['A', 'B', 'C', 'D', 'E'], # 设置饼图标签\n    colors=['k', 'r', 'y', 'm', 'c'], # 设置饼图颜色\n    explode=(0, 0.3, 0, 0, 0), # 第二部分突出显示，值越大，距离中心越远\n    autopct='%.2f%%', # 格式化输出百分比\n)\nplt.show()\n\n\n\n","slug":"Matplotlib常用方法","date":"2022-06-30T13:27:12.000Z","categories_index":"机器学习,深度学习,Matplotlib","tags_index":"机器学习,深度学习,Matplotlib","author_index":"Elijah"},{"id":"def2b3edec673bcc6047d2d81dfd1856","title":"Git常见问题-Deployer not found git","content":"npm install hexo-deployer-git --save","slug":"Git常见问题-Deployer not found git","date":"2022-06-28T14:17:00.000Z","categories_index":"Git,Hexo","tags_index":"Git,Hexo","author_index":"Elijah"},{"id":"9e6e6987ef0095efdfe7b551204f80c4","title":"Git常见问题-OpenSSL SSL_read Connection was reset","content":"git config --global http.sslVerify &quot;false&quot;","slug":"Git常见问题-OpenSSL SSL_read Connection was reset","date":"2022-06-28T14:14:46.000Z","categories_index":"Git,Hexo","tags_index":"Git,Hexo","author_index":"Elijah"},{"id":"b59fc62089fc87143e9d813fcf584fc0","title":"树莓派 PICO基础教程（基于MicroPython）","content":"树莓派 PICO基础教程（基于MicroPython）1 树莓派 PICO 简介1.1 简介　　Raspberry Pi Pico是具有灵活数字接口的低成本，高性能微控制器板。它集成了Raspberry Pi自己的RP2040微控制器芯片，运行速度高达133 MHz的双核Arm Cortex M0 +处理器，嵌入式264KB SRAM和2MB板载闪存以及26个多功能GPIO引脚。对于软件开发，可以使用Raspberry Pi的C &#x2F; C ++ SDK或MicroPython。^1\n\n\n1.2 配置 ^2\n\n\n树莓派 PICO配置\n\n\n\n双核 Arm Cortex-M0 + @ 133MHz\n\n\n2 个 UART、2 个 SPI 控制器和 2 个 I2C 控制器\n\n\n芯片内置 264KB SRAM 和 2MB 的板载闪存\n\n\n16 个 PWM 通道\n\n\n通过专用 QSPI 总线支持最高 16MB 的片外闪存\n\n\nUSB 1.1 主机和设备支持\n\n\nDMA 控制器\n\n\n8 个树莓派可编程 I&#x2F;O（PIO）状态机，用于自定义外围设备支持\n\n\n30 个 GPIO 引脚，其中 4 个可用作模拟输入\n\n\n支持 UF2 的 USB 大容量存储启动模式，用于拖放式编程\n\n\n1.3 引脚图\n1.4 尺寸\n\n\n2 安装2.1 烧录固件\n点击 https://micropython.org/download/rp2-pico/rp2-pico-latest.uf2 链接下载UF2文件；\n\n\n\n\n\n\n\n\n\n\n如果连接失效，可以进入 https://www.raspberrypi.org/documentation/rp2040/getting-started/#getting-started-with-micropython官网下载\n\n按住BOOTSEL键不放，将Pico插入电脑的USB串口，电脑上会弹出一个新的U盘文件夹，把刚刚下载的UF2文件拖拽到文件夹中，树莓派 PICO将会自动重启，此时，固件烧录完成。\n\n2.2 安装IDE（Thonny IDE）\n进入软件官网 https://thonny.org/下载软件，最好下载最新版的，否则可能不支持树莓派 PICO；\n安装Thonny，安装完成后打开Thonny软件，打开工具-&gt;设置-&gt; 解释器，选择MicroPython(Raspberry Pi Pico)解释器，并在串口处选择树莓派PICO的串口号（如果板子已经连接在电脑上，软件一般会自动检测串口号）\n重启软件，可以看到软件左下方显示了树莓派PICO中的文件；\n\n\n\n\n\n\n\n\n\n\n如果没有显示左侧文件树的话可以勾选 视图-&gt;文件\n2.3 离线运行程序\n　　新建文件，编写完代码后，按住ctrl+s将该文件保存在树莓派PICO上，并命名为main.py(一定要加后缀.py)，下次树莓派PICO通电时便会自动运行main.py中的程序。\n3 基础3.01 点亮板载LED灯from machine import Pin\n\nif __name__ == '__main__':\n    # 构建led对象\n    # 板载LED灯连接与引脚25相连\n    # LED = Pin(id, mode, pull)\n    # id:PICO引脚编号\n    # mode:输入输出方式，有Pin.IN(输入)和Pin.OUT(输出)两种\n    # pull:上下拉电阻配置，有None(无上下拉电阻)、Pin.PULL_UP(上拉电阻)和Pin.PULL_DOWN(下拉电阻)三种\n    LED = Pin(25, Pin.OUT)\n    # 高电平点亮\n    LED.value(1)\n\n3.02 板载LED闪烁from machine import Pin\nfrom utime import sleep\nimport utime\n\nled = Pin(25, Pin.OUT)\n\nif __name__ == '__main__':\n    while True:\n        # led点亮\n        led.value(1)\n        utime.sleep_ms(1000)\n        # led熄灭\n        led.value(0)\n        utime.sleep_ms(1000)\n\n3.03 LED流水灯\nLED发光二极管图片\n\n\n\nLED发光二极管正负极区分\n\n\n一般引脚长的一端为正极，引脚短的为负极\n看发光二极管内部，支架大的为负极，支架小的为负极\n\n\n电路连线图\n\n\n\n\n代码\n\nfrom machine import Pin\nimport utime\n\n# 定义LED引脚数组\nleds = [Pin(i,Pin.OUT) for i in range(0,5)]\n\nif __name__ == '__main__':\n    while True:\n        # 依次点亮\n        for n in range(0,5):\n            leds[n].value(1)\n            utime.sleep_ms(200)\n        # 依次熄灭\n        for n in range(0,5):\n            leds[n].value(0)\n            utime.sleep_ms(100)\n\n\n3.04 按键实验\n四角按键图片\n\n\n\n四角按键怎么连接\n\n\n默认按键未按下的情况下，12相连接，34相连接；当按下按键时，1234才相连接。\n\n电路接线图\n\n\n\n\n代码\n\nfrom machine import Pin\nimport utime\n\n# 配置按键\n# key = machine.Pin(id, mode, pull)\n# id:树莓派Pico引脚编号\n# mode:输入输出方式，有Pin.IN(输入)和Pin.OUT(输出)两种\n# pull:上下拉电阻配置，有None(无上下拉电阻)、Pin.PULL_UP(上拉电阻)和Pin.PULL_DOWN(下拉电阻)三种\nkey = Pin(0, Pin.IN, Pin.PULL_UP)\n\nif __name__ == '__main__':\n    while True:\n        # print(key.value())\n        if key.value() == 0:\n            # 等待一段时间，防止抖动\n            utime.sleep_ms(100)\n            if key.value() == 0:\n                print('The button is pressed')\n\n\n\n\n\n\n\n\n\n\n按键消抖可以参考https://baike.baidu.com/item/%E6%8C%89%E9%94%AE%E6%B6%88%E6%8A%96\n3.05 外部中断(改进3.04 按键实验)\n什么是外部中断\n\n　　外部中断是单片机实时地处理外部事件的一种内部机制。当某种外部事件发生时，单片机的中断系统将迫使CPU暂停正在执行的程序，转而去进行中断事件的处理；中断处理完毕后．又返回被中断的程序处，继续执行下去。^3\n\n外部中断的作用\n\n\n节省CPU资源\n\n\n代码实现\n\n　　在3.04 按键实验中，检测按键是否被按下采用的是在主程序中写死循环的办法，假如这个按键被按下的频率十分低（一天只有几次被按下），采用死循环的方法将会浪费大量的CPU资源，而采用外部中断的方式检测按键是否被按下将大大节省CPU资源。\nfrom machine import Pin\nimport utime\n\n#配置按键\nkey = Pin(0, Pin.IN, Pin.PULL_UP)\n\ndef external_interrupt(key):\n    # 消除抖动\n    utime.sleep_ms(100)\n    # 再次判断按键是否被按下\n    if key.value() == 0:\n        print('The button is pressed')\n\nif __name__ == '__main__':\n    # KEY.irq(handler,trigger)\n    # handler:中断执行的回调函数\n    # trigger:触发中断的方式，分别为Pin.IRQ_FALLING(下降沿触发)、\n    # Pin.IRQ_RISING(上升沿触发)、Pin.IRQ_LOW_LEVEL(低电平触发)和\n    # Pin.IRQ_HIGH_LEVEL(高电平触发)四种\n    # 定义中断，下降沿触发\n    key.irq(external_interrupt, Pin.IRQ_FALLING)\n\n3.06 定时器中断(改进3.02 板载LED闪烁)\n什么是定时器中断\n\n\n定时器中断是由单片机中的定时器溢出而申请的中断，即设定一个时间，到达这个时间后就会产生中断\n\n\n代码\n\n通过设置定时器中断使树莓派PICO板载LED每隔两秒闪烁一次\nfrom machine import Pin, Timer\n\n# 创建LED对象\nled=Pin(25, Pin.OUT)\n\n# 闪烁回调函数\ndef twinkle(tim):\n    # toggle方法:LED状态翻转\n    led.toggle()\n\nif __name__ == '__main__':\n    # 构建定时器\n    tim = Timer()\n    # tim.init(period, mode, callback)\n    # period:周期时间(单位为ms)\n    # mode:工作模式，有Timer.ONE_SHOT(执行一次)和Timer.PERIODIC(周期性执行)两种\n    # callback:定时器中断的回调函数\n    tim.init(period=2000, mode=Timer.PERIODIC, callback=twinkle)\n\n3.07 PWM 脉冲宽度调制(实现板载LED呼吸灯)\n什么是PWM\n\n　　脉冲宽度调制是一种模拟控制方式，根据相应载荷的变化来调制晶体管基极或MOS管栅极的偏置，来实现晶体管或MOS管导通时间的改变，从而实现开关稳压电源输出的改变。这种方式能使电源的输出电压在工作条件变化时保持恒定，是利用微处理器的数字信号对模拟电路进行控制的一种非常有效的技术。脉冲宽度调制是利用微处理器的数字输出来对模拟电路进行控制的一种非常有效的技术，广泛应用在从测量、通信到功率控制与变换的许多领域中。^4\n\n代码\n\nfrom machine import Pin, Timer, PWM\nimport utime\n\nled = PWM(Pin(25))\n# 设置频率值\nled.freq(1000)\n\nled_value = 0\n# led以5%增长/减少的速度变化亮度\nled_space = 5\n\nif __name__ == '__main__':\n    while True:\n        led_value += led_space\n        if led_value >= 100:\n            led_value = 100\n            led_space = -5\n        elif led_value &lt;= 0:\n            led_value = 0\n            led_space = 5\n        # 设置占空比，需在0-65535之间\n        led.duty_u16(int(led_value * 500))\n        utime.sleep_ms(100)\n\n\n3.08 I2C总线(使用SSD1306 OLED屏幕)\nI2C总线简介\n\n　　I2C总线是由Philips公司开发的一种简单、双向二线制同步串行总线。它只需要两根线即可在连接于总线上的器件之间传送信息。I2C由 2 条线组成：SDA（串行数据线）和SCL（串行时钟线），都是双向I&#x2F;O线。^5\n\nSSD1306 OLED简介\n\n　　SSD1306是一款带控制器的用于OLED点阵图形显示系统的单片CMOS OLED&#x2F;PLED驱动器。它由128个SEG（列输出）和64个COM（行输出）组成。该芯片专为共阴极OLED面板设计。  SSD1306内置对比度控制器、显示RAM（GDDRAM）和振荡器，以此减少了外部元件的数量和功耗。该芯片有256级亮度控制。数据或命令由通用微控制器通过硬件选择的6800&#x2F;8000系通用并行接口、I2C接口或串行外围接口发送。该芯片适用于许多小型便携式应用，如手机副显示屏、MP3播放器和计算器等。^6\n\n电路连线图\n\n\n\n代码\n\n\n\n\n\n\n\n\n\n\nssd1306.py下载地址： https://elijah.lanzoui.com/iJ13fpnq6je\n下载完成后，在Thonny软件左侧的文件窗口内找到这个文件，右键点击文件，选择上载到选项，文件即可传输到树莓派PICO上\nfrom machine import SoftI2C, Pin\n# 导入SSD1306驱动模块\nfrom ssd1306 import SSD1306_I2C\n\nif __name__ == '__main__':\n    # 初始化SoftI2C\n    # OLED屏幕的scl连接到树莓派PICO的GPIO0, sda连接到GPIO1\n    i2c = SoftI2C(scl=Pin(0), sda=Pin(1))\n    # oled = SSD1306_I2C(width, height, i2c, addr)\n    # width:屏幕宽\n    # height: 屏幕高\n    # i2c:已定义的I2C对象\n    oled = SSD1306_I2C(128, 64, i2c) #OLED显示屏初始化：128*64分辨率,OLED的I2C地址是0x3c\n    # OLED显示的字符串，横坐标和纵坐标\n    oled.text(\"Hello World!\", 0, 0)\n    # OLED显示\n    oled.show()\n\n\n4 传感器程序4.1 温度传感器(DS18B20)DS18B20是常用的数字温度传感器，其输出的是数字信号，具有体积小，硬件开销低，抗干扰能力强，精度高的特点。\n\n测温范围: -55℃～+125℃，固有测温误差1℃\n\n工作电源: 3.0~5.5V&#x2F;DC\n\n单总线驱动，只占用一个IO口\n\n\nimport machine, onewire, ds18x20, time, utime\n\n# 使用GPIO0口传输数据\n# 将DS18B20的VCC端连接到树莓派PICO的3V3(OUT)端\n# 将DS18B20的数据端连接到树莓派PICO的GPIO0口\n# 将DS18B20的GND端连接到树莓派PICO的GND端\npin = machine.Pin(0)\nsensor = ds18x20.DS18X20(onewire.OneWire(pin))\n\n# 扫描是否存在DS18B20设备\nroms = sensor.scan()\nprint('Found a ds18x20 device')\n\n# 获取温度数据\ndef detect_tem():\n    while True:\n      sensor.convert_temp()\n      for rom in roms:\n        # 打印出温度值\n        # 第一个打印出来的数值可能不太准确，从第二条数据开始才会显示出正常数据\n        print(\"&#123;:.3f&#125;\".format(sensor.read_temp(rom)))  \n      utime.sleep_ms(2000)\n      \n# 程序入口\nif __name__ == '__main__':\n    detect_tem()\n\n4.2 温湿度传感器\n\n\n\n\n\n\n\n\nDHT22.py文件下载地址： https://elijah.lanzoui.com/iFueapnq6id\n文件上传方法参考3.08 I2C总线\n4.2.1 DHT11DHT11是一款有已校准数字信号输出的温湿度传感器。 其精度湿度±5%RH， 温度±2℃，量程湿度5-95%RH， 温度0-+50℃。^9\nfrom machine import Pin\nfrom DHT22 import DHT22\nimport utime\n\npin = Pin(0,Pin.IN,Pin.PULL_UP)\n# 创建dht11对象\n# 将DHT11的VCC端连接到树莓派PICO的3V3(OUT)端\n# 将DHT11的数据端连接到树莓派PICO的GPIO0口\n# 将DHT11的GND端连接到树莓派PICO的GND端\ndht_sensor=DHT22(pin, dht11=True)\n\n# 循环函数\ndef detection():\n    while True:\n        T, H = dht_sensor.read()\n        if T is None:\n            print(\"sensor error\")\n        else:\n            print(\"&#123;&#125;'C  &#123;&#125;%\".format(T, H))\n        utime.sleep_ms(2000)\n\n# 程序入口\nif __name__ == '__main__':    \n    detection()\n\n4.2.1 DHT22　　DHT22也称AM2302，是一款含有已校准数字信号输出的温湿度复合传感器，湿度量程范围0-99.9%RH，精度±2%RH，而温度量程范围是-40℃-80℃，精度±0.5℃。^10\nfrom machine import Pin\nfrom DHT22 import DHT22\nimport utime\n\npin = Pin(0,Pin.IN,Pin.PULL_UP)\n# 创建dht11对象\n# 将DHT11的VCC端连接到树莓派PICO的3V3(OUT)端\n# 将DHT11的数据端连接到树莓派PICO的GPIO0口\n# 将DHT11的GND端连接到树莓派PICO的GND端\ndht_sensor=DHT22(pin, dht11=False)\n\n# 循环函数\ndef detection():\n    while True:\n        T, H = dht_sensor.read()\n        if T is None:\n            print(\"sensor error\")\n        else:\n            print(\"&#123;:.2f&#125;'C  &#123;:.2f&#125;%\".format(T, H))\n        utime.sleep_ms(2000)\n\n# 程序入口\nif __name__ == '__main__':    \n    detection()\n\n\n\n","slug":"树莓派 PICO基础教程（基于MicroPython）","date":"2022-06-28T14:01:30.000Z","categories_index":"MCU,树莓派pico,MicroPython","tags_index":"MCU,树莓派pico,MicroPython","author_index":"Elijah"}]